---
title: "P8451_HW6_ML"
author: "Ruixi Li"
date: "2024-02-20"
output: html_document
---

```{r library, message=FALSE, warning=FALSE}
library(NHANES)
library(tidyverse)
library(caret)
library(rpart.plot)
library(pROC)
```

# 1. Load data

```{r load_data}
data(NHANES) 



```


# 2. select features and partition data

* Although classification tree and support vector machine are both non-parametric method and don't assume the independence of features, interpretation might be faulty since dependence/correlation between features could be intrepreted as an interaction effect. This will not effect prediction. So, I would drop weight and height, two of which were used to calculate BMI. But theoretically, BMI is more correlated with diabetes and pre-diabetes. Although it's believed that BMI and physical activity are correlated, I think physical activity have some unexplained effect towards diabetes and should be kept as a predictor.

```{r select_features_and_patittion, message=FALSE, warning=FALSE}
# feature selection and remove duplicate rows
NHANES_tailored = NHANES|>
  select(Age, Race1, Education, HHIncome, Pulse, Diabetes, BMI, PhysActive, Smoke100) |> janitor::clean_names() |> distinct()

# investigation and light cleaning
NHANES_tailored|> skimr::skim()

NHANES_tailored |> Amelia::missmap(main = "Missing values vs observed")

NHANES_tailored = NHANES_tailored |> drop_na()

NHANES_tailored |> Amelia::missmap(main = "Missing values vs observed")

#tidyverse way to create data partition 
set.seed(123)

train.indices = NHANES_tailored |>
  pull(diabetes) |>
  createDataPartition(p = 0.7, list = FALSE)

train.data = NHANES_tailored |>
  slice(train.indices)

test.data = NHANES_tailored |>
  slice(-train.indices)

summary(NHANES_tailored$diabetes)
# Note that data are slightly unbalanced.
```


# 3. Model building

## classification tree

```{r model_building_tree}
set.seed(123)
#Creating 10-fold cross-validation and using down-sampling because of imbalance in data
control= trainControl(method="cv", number=10, sampling="down", classProbs = T)

# Up-sampling is preferred when the dataset is small, and data loss is a concern, while down-sampling might be more appropriate for large datasets where training time and resource utilization are significant considerations.


# classification tree

modelLookup("rpart")

#Specify tuneGrid so caret explores wider variety of cp-values
set.seed(123)

#Create different values of cp to try
cp.grid=expand.grid(cp=seq(0.001,0.1, by=0.001))
tree=train(
                     diabetes ~ ., 
                     data=train.data, 
                     method="rpart", 
                     trControl=control, 
                     tuneGrid=cp.grid,
                     preProcess=c("center", "scale")
                     )

plot(tree, uniform=TRUE)
tree$bestTune
tree$results

#Plot new "best" tree
tree$finalModel |>
  rpart.plot()

#Example variable importance in model
varImp(tree)


```

## Support Vector Machine

```{r model_building_svm}
library(e1071)
modelLookup("svmLinear")
set.seed(123)
#Incorporate different values for cost (C)
svm=train(
                    diabetes ~ ., 
                    data=train.data, 
                    method="svmLinear",  
                    trControl=control, 
                    preProcess=c("center", "scale"), 
                    tuneGrid=expand.grid(C=seq(0.001,2, length=30))
                    )

#Visualize accuracy versus values of C
plot(svm)

svm$bestTune
#See information about final model
svm$finalModel

```

## Logistic Regression

```{r model_building_log}
set.seed(123)

logistic = train(diabetes ~ ., 
                  data = train.data, method = "glm",
                  family = "binomial",
                  trControl = control,
                  preProcess=c("center", "scale"))



```

# 4. Model selection and evaluation 

```{r model_evaluation_tree, message=FALSE, warning=FALSE}

#Obtain metrics of accuracy from training
confusionMatrix(tree)
confusionMatrix(svm)
confusionMatrix(logistic)

```

* Since the accuracy for Support Vector Machine is the highest(0.7213), I would chose Support Vector Machine as my optimal model.

## Support Vector Machine

```{r model_evaluation_svm, message=FALSE, warning=FALSE}

#Create predictions in test set
pred.svm = svm |>
              predict(test.data)

eval.results=confusionMatrix(pred.svm, test.data$diabetes, positive = "Yes")
print(eval.results)

#Create predictions as probabilities on test set 
pred.svm.prob = svm |> 
  predict(test.data, type = "prob")

#Another potential evaluation: Area under the Receiver Operating Curve (AUROC)
analysis = roc(response=test.data$diabetes, predictor=pred.svm.prob[,2])
plot(1-analysis$specificities,analysis$sensitivities,type="l",
ylab="Sensitivity",xlab="1-Specificity",col="red",lwd=2,
main = "ROC Curve for three models")
abline(a=0,b=1)
```

* The model appears to have a fair capacity to identify the positive class(sensitivity/recall=79.72%) but struggles with precision(precision=25.39%), indicating a high rate of false positives. This is also suggested by the low positive predictive value. The model is not outperforming a naive classifier that would predict the most frequent class for all cases, as evidenced by the comparison to the NIR and the associated p-value. The balanced accuracy is 74.18%, which  is a better metric than accuracy for imbalanced datasets.

# 5. Ethical consideration of using race as a predictor

* Including race as a predictor in disease prediction models raises significant ethical considerations, reflecting broader societal issues around equity, discrimination, and the interpretation of genetic versus socio-environmental determinants of health. It may divert attention from underlying causes of health disparities, such as socio-economic status, physical activity and BMI which are more directly actionable. 



